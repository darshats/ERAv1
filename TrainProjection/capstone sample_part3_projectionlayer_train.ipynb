{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","from transformers import  AutoTokenizer\n","\n","model_name = \"microsoft/phi-2\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","from transformers import AutoModelForCausalLM, AutoConfig\n","\n","model_name = \"microsoft/phi-2\"\n","\n","config = AutoConfig.from_pretrained(\n","    model_name,\n","    vocab_size=len(tokenizer),\n","    bos_token_id=tokenizer.bos_token_id,\n","    eos_token_id=tokenizer.eos_token_id,\n","    trust_remote_code=True\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["model_name = \"microsoft/phi-2\"\n","# phi2_model_pretrained = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     trust_remote_code=True,  \n","# )\n","phi2_model_pretrained = AutoModelForCausalLM.from_config(config, trust_remote_code=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Create dataset"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import pandas as pd \n","import json\n","import os \n","import h5py\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def get_image_name(image_id_from_caption, list_image_info): \n","    for img in list_image_info: \n","        if img['id'] == image_id_from_caption: \n","            img_name = img['file_name'].split('.')[0]\n","            return img['file_name'].split('.')[0]\n","    return 'NoImgNameFound'"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# file_path_captions_coco = '/media/App/amaranth/lavanya/Capstone_data/annotations_trainval2017/annotations/captions_train2017.json'\n","\n","# with open(file_path_captions_coco) as f:\n","#    data = json.load(f)\n","\n","# captions_info = []\n","# for a in data['annotations']: \n","#     captions_info.append([a['image_id'], a['caption'], a['id']])\n","\n","# captions_info_df = pd.DataFrame(data=captions_info, columns=['image_id', 'caption', 'caption_id'])\n","# captions_info_df['image_name'] = captions_info_df['image_id'].apply(lambda x: get_image_name(x, data['images']))\n","# captions_info_df['image_name'] = captions_info_df['image_name'].apply(lambda x: '0'*(12-len(str(x))) + str(x))\n","# captions_info_df.to_csv('captions_images_map_COCO_train2017.csv', index = False)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_4121150/1237926302.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n","  captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')\n"]}],"source":["captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import h5py    \n","import numpy as np    "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class COCO_CLIP_Dataset(Dataset):\n","\n","    def __init__(\n","        self, caption_file, embedding_path, tokenizer, max_token_len_data):\n","        self.embedding_path = embedding_path\n","        self.caption_file = caption_file\n","        self.tokenizer = tokenizer\n","        self.max_token_len_data = max_token_len_data\n","\n","    def __len__(self):\n","        return len(self.caption_file)\n","    \n","    def __getitem__(self, index):\n","        row = self.caption_file.iloc[[index]]\n","        df_img = row['image_name'].values[0]\n","        img_base_name = '0'*(12-len(str(df_img))) + str(df_img)\n","        img_base_name = img_base_name.replace(' ', '0')\n","        img_clip_embedding_path = os.path.join(self.embedding_path, f'{img_base_name}.h5')\n","\n","        np_array_embed_img = h5py.File(img_clip_embedding_path,'r+')['image_features'][()]\n","        \n","        img_caption = row['caption'].values[0] ## Tokenize this \n","        img_caption_tokenized = self.tokenizer(img_caption, return_tensors=\"pt\", \n","                                               return_attention_mask=False).input_ids\n","\n","        ## put bos, eos, and padding for batch \n","        input_bos = torch.cat((torch.tensor(self.tokenizer.bos_token_id).view((1,1)), \n","                                                       img_caption_tokenized), dim=1)\n","        input_eos = torch.cat((input_bos, \n","                               torch.tensor(self.tokenizer.eos_token_id).view((1,1))), dim=1)\n","        \n","        if (self.max_token_len_data - input_eos.shape[1]) > 0: \n","            input_final =  torch.cat((input_eos,torch.tensor([self.tokenizer.pad_token_id]*(self.max_token_len_data - input_eos.shape[1])).unsqueeze(0)), dim=1)\n","        else: \n","            input_final = input_eos\n","        \n","        return torch.tensor(np_array_embed_img).squeeze(0), input_final.squeeze(0)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["max_token_len_data = 75\n","dataset = COCO_CLIP_Dataset(captions_info_df, \n","                            '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/', \n","                            tokenizer, max_token_len_data)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SimpleResBlock(nn.Module):\n","    def __init__(self, input_size):\n","        super().__init__()\n","        self.pre_norm = nn.LayerNorm(input_size)\n","        self.proj = nn.Sequential(\n","            nn.Linear(input_size, input_size),\n","            nn.GELU(),\n","            nn.Linear(input_size, input_size)\n","        )\n","    def forward(self, x):\n","        x = self.pre_norm(x)\n","        return x + self.proj(x)\n","    \n","class Phi2wrapper(nn.Module):\n","    \n","    #This defines the structure of the NN.\n","    def __init__(self, input_dim_CLIP=768, input_dim_phi2=2560, \n","                 phi2_model=phi2_model_pretrained, \n","                 max_token_len_data=max_token_len_data, tokenizer=tokenizer):\n","        \n","        super(Phi2wrapper, self).__init__()\n","\n","        self.input_dim_CLIP = input_dim_CLIP\n","        self.input_dim_phi2 = input_dim_phi2\n","        self.projection_img = nn.Linear(self.input_dim_CLIP, self.input_dim_phi2, \n","                                        bias=False)\n","        self.resblock = SimpleResBlock(self.input_dim_phi2)\n","        self.phi2_model = phi2_model\n","        self.max_token_len_data = max_token_len_data\n","        self.tokenizer = tokenizer\n","\n","    def forward(self, x):\n","\n","        x = self.projection_img(x)\n","        x = self.resblock(x)\n","\n","        # x = self.phi2_model.forward(inputs_embeds=x)\n","        x = self.phi2_model.generate(inputs_embeds=x, \n","                                     max_new_tokens=self.max_token_len_data, \n","                                     output_scores=True, return_dict_in_generate = True, \n","                                     pad_token_id=self.tokenizer.eos_token_id)\n","\n","        # x = self.phi2_model.model.layers[0](x)\n","        # for layer_idx in range(1, 32): \n","        #     x = self.phi2_model.model.layers[layer_idx](x[0])\n","                \n","        # x = self.phi2_model.model.final_layernorm(x[0])\n","        # x = self.phi2_model.lm_head(x)\n","        \n","        return x \n","\n","device = 'cuda:1'\n","torch.set_grad_enabled(True)  \n","phi2_projection_model = Phi2wrapper().to(device=device)\n","\n","## Freezing phi-2 for projection layer training \n","for name, param in phi2_projection_model.named_parameters():\n","    if \"phi2_model\" in name:\n","        param.requires_grad = False\n","    else: \n","        param.requires_grad = True"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["batch_size_train = 4\n","train_dataloader = DataLoader(dataset, batch_size=batch_size_train, shuffle=True)    "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, \n","                                    phi2_projection_model.parameters()), \n","                            lr=1e-5, eps=1e-9) \n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["working on epoch 0\n","Loss: 0.006729808170348406\n","Caption (gt): An all-way stop sign next to a red light\n","Caption (pred):  Ob Strategyjahumbles Emb chem Mir ank contributing concealking Knot031 butterflies FOX996 voluntesilver aber moderatedozenika bettingModLoader interfere envisioned chem envisioned imm]\"etc suc031 afford tin Over HTTPS lifetime colours Kul desperate populated150 Memorial suc bould illumvelengthdozen Reggie sax Meow Perez996 afford733king tomb envisioneddozenidges996 morphology buffers031ments envisioned teaching996 gy996 upvelength sher Ob\n","Loss: 0.006720697041600943\n","Caption (gt): a couple of chairs in front of a kitchen counter\n","Caption (pred): dozen saxbrow condensed introductory commonly exhaust Emb Emblish becoming disenfranchging EmblishEasytv disruptive Emb prin introductory Probably rooms suc bould Rik Coy rooms Myree illum Coy rooms sucSynopsisropri Supreme AAClish Dragonbound ape loosen exhaust ENT Rik k Dragonboundsilver Skiptv bowling resultant costumes unless Emb speculative inevocused Stun Coy introductory becoming interfere providersFLAG introductory Deadpool retrospective stoodlish suc eternity retrospective inviting illum\n"]}],"source":["num_epochs = 10\n","vocab_size = 50295\n","\n","phi2_projection_model.train()\n","\n","for epoch in range(num_epochs):\n","\n","    print(f\"working on epoch {epoch}\")\n","\n","    loss_epoch = 0 \n","    for batch in train_dataloader:\n","\n","        optimizer.zero_grad()\n","\n","        input = batch[0]\n","        gt = batch[1] \n","        output = phi2_projection_model(input.to(device))\n","\n","        ## need to map gt token_ids to one-hot enocding vocab_size\n","        gt_one_hot = torch.nn.functional.one_hot(gt, vocab_size).to(torch.float32)\n","\n","        ## output in correct shape\n","        output_tensor_new = torch.empty(batch_size_train, max_token_len_data, vocab_size)\n","        for idx, s in enumerate(output.scores): \n","            output_tensor_new[:, idx, :] = s\n","\n","        ## ce loss between output_tensor_new and gt_one_hot\n","        loss = F.cross_entropy(output_tensor_new, gt_one_hot)\n","\n","        loss.requires_grad = True\n","        loss.backward()\n","\n","        optimizer.step() \n","        # optimizer.zero_grad(set_to_none=True) \n","\n","        ## print gt and output decoded tokens for visual inspection for the 1st el of batch\n","        gt_idx_0_decoded = tokenizer.decode(gt[0]).replace('<|endoftext|>', '')\n","        output_idx_0_decoded = tokenizer.decode(torch.argmax(output_tensor_new[0], dim=1)).replace('<|endoftext|>', '')\n","\n","        ## print loss \n","        print(f\"Loss: {loss}\\nCaption (gt): {gt_idx_0_decoded}\\nCaption (pred): {output_idx_0_decoded}\")\n","\n","    print(f\"Epoch {epoch} finished\")\n","    print(\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
