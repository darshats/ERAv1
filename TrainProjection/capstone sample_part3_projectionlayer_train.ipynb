{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"248d7f4dc5d44278814cd5c9d1ea5f6b","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"microsoft/phi-2\"\n","phi2_model_pretrained = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Create model "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Phi2wrapper(nn.Module):\n","    \n","    #This defines the structure of the NN.\n","    def __init__(self, input_dim_CLIP=768, input_dim_phi2=2560, phi2_model=phi2_model_pretrained):\n","        super(Phi2wrapper, self).__init__()\n","        self.input_dim_CLIP = 768\n","        self.input_dim_phi2 = 2560\n","        self.projection_img = nn.Linear(self.input_dim_CLIP, self.input_dim_phi2, \n","                                        bias=False)\n","        self.phi2_model = phi2_model\n","\n","    def forward(self, x):\n","        x = self.projection_img(x)\n","        x = self.phi2_model(x)\n","        return x "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["phi2_projection_model = Phi2wrapper()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["## Freezing phi-2 for projection layer training \n","\n","for name, param in phi2_projection_model.named_parameters():\n","    if \"phi2_model\" in name:\n","        param.requires_grad = False"]},{"cell_type":"markdown","metadata":{},"source":["### Create dataset"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import pandas as pd \n","import json\n","import os \n","import h5py\n"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["def get_image_name(image_id_from_caption, list_image_info): \n","    for img in list_image_info: \n","        if img['id'] == image_id_from_caption: \n","            img_name = img['file_name'].split('.')[0]\n","            return img['file_name'].split('.')[0]\n","    return 'NoImgNameFound'"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["# file_path_captions_coco = '/media/App/amaranth/lavanya/Capstone_data/annotations_trainval2017/annotations/captions_train2017.json'\n","\n","# with open(file_path_captions_coco) as f:\n","#    data = json.load(f)\n","\n","# captions_info = []\n","# for a in data['annotations']: \n","#     captions_info.append([a['image_id'], a['caption'], a['id']])\n","\n","# captions_info_df = pd.DataFrame(data=captions_info, columns=['image_id', 'caption', 'caption_id'])\n","# captions_info_df['image_name'] = captions_info_df['image_id'].apply(lambda x: get_image_name(x, data['images']))\n","# captions_info_df['image_name'] = captions_info_df['image_name'].apply(lambda x: '0'*(12-len(str(x))) + str(x))\n","# captions_info_df.to_csv('captions_images_map_COCO_train2017.csv')"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_3065095/1237926302.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n","  captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')\n"]}],"source":["captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["import h5py    \n","import numpy as np    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class COCO_CLIP_Dataset(Dataset):\n","    def __init__(\n","        self, caption_file, embedding_path):\n","        self.embedding_path = embedding_path\n","        self.caption_file = caption_file\n","\n","    def __len__(self):\n","        return len(self.caption_file)\n","    \n","    def __getitem__(self, index):\n","        row = self.caption_file.iloc[[index]]\n","\n","        img_base_name = '0'*(12-len(str(row['image_id']))) + str(row['image_id'])\n","        img_clip_embedding_path = os.path.join(self.embedding_path, f'{img_base_name}.h5')  \n","        np_array_embed_img = h5py.File(img_clip_embedding_path,'r+')['image_features'][()]\n","        \n","        img_caption = row['caption'] ## Tokenize this \n","\n","        return torch.tensor(np_array_embed_img), img_caption"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
