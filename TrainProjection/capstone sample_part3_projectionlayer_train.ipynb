{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","from transformers import  AutoTokenizer\n","from transformers import AutoModelForCausalLM, AutoConfig\n","\n","model_name = \"microsoft/phi-2\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","# tokenizer.pad_token = tokenizer.eos_token\n","# tokenizer.bos_token = tokenizer.eos_token\n","\n","# config = AutoConfig.from_pretrained(\n","#     model_name,\n","#     vocab_size=len(tokenizer),\n","#     bos_token_id=tokenizer.bos_token_id,\n","#     eos_token_id=tokenizer.eos_token_id,\n","#     trust_remote_code=True\n","# )\n","\n","# phi2_model_pretrained = AutoModelForCausalLM.from_config(config, \n","#                                                          trust_remote_code=True, \n","#                                                          )"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2b711a59af84ae79ecdff32578f09d1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"microsoft/phi-2\"\n","phi2_model_pretrained = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True,  \n",")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have added 2 tokens\n"]},{"data":{"text/plain":["Embedding(50297, 2560)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["special_tokens_dict = {'pad_token': '<|PAD|>', 'bos_token': '<|BOS|>'}\n","\n","num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n","print('We have added', num_added_toks, 'tokens')\n","phi2_model_pretrained.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer."]},{"cell_type":"markdown","metadata":{},"source":["### Create dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import pandas as pd \n","import json\n","import os \n","import h5py"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_image_name(image_id_from_caption, list_image_info): \n","    for img in list_image_info: \n","        if img['id'] == image_id_from_caption: \n","            img_name = img['file_name'].split('.')[0]\n","            return img['file_name'].split('.')[0]\n","    return 'NoImgNameFound'"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# file_path_captions_coco = '/media/App/amaranth/lavanya/Capstone_data/annotations_trainval2017/annotations/captions_train2017.json'\n","\n","# with open(file_path_captions_coco) as f:\n","#    data = json.load(f)\n","\n","# captions_info = []\n","# for a in data['annotations']: \n","#     captions_info.append([a['image_id'], a['caption'], a['id']])\n","\n","# captions_info_df = pd.DataFrame(data=captions_info, columns=['image_id', 'caption', 'caption_id'])\n","# captions_info_df['image_name'] = captions_info_df['image_id'].apply(lambda x: get_image_name(x, data['images']))\n","# captions_info_df['image_name'] = captions_info_df['image_name'].apply(lambda x: '0'*(12-len(str(x))) + str(x))\n","# captions_info_df.to_csv('captions_images_map_COCO_train2017.csv', index = False)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_390507/1237926302.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n","  captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')\n"]}],"source":["captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import h5py    \n","import numpy as np    "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class COCO_CLIP_Dataset(Dataset):\n","\n","    def __init__(\n","        self, caption_file, embedding_path, tokenizer, max_token_len_data):\n","        \n","        self.embedding_path = embedding_path\n","        self.caption_file = caption_file\n","        self.tokenizer = tokenizer\n","        self.max_token_len_data = max_token_len_data\n","\n","    def __len__(self):\n","        return len(self.caption_file)\n","    \n","    def __getitem__(self, index):\n","\n","        row = self.caption_file.iloc[[index]]\n","\n","        df_img = row['image_id'].values[0]\n","        img_base_name = '0'*(12-len(str(df_img))) + str(df_img)\n","        img_base_name = img_base_name.replace(' ', '0')\n","        img_clip_embedding_path = os.path.join(self.embedding_path, f'{img_base_name}.h5')\n","\n","        np_array_embed_img = h5py.File(img_clip_embedding_path,'r+')['image_features'][()]\n","        \n","        img_caption = row['caption'].values[0] ## Tokenize this \n","        img_caption_tokenized = self.tokenizer(img_caption, return_tensors=\"pt\", \n","                                               return_attention_mask=False).input_ids\n","\n","        ## put bos, eos, and padding for batch         \n","        input_bos = torch.cat((torch.tensor(self.tokenizer.bos_token_id).view((1,1)), \n","                                                       img_caption_tokenized), dim=1)\n","\n","        input_bos = img_caption_tokenized\n","\n","        input_eos = torch.cat((input_bos, \n","                               torch.tensor(self.tokenizer.eos_token_id).view((1,1))), dim=1)\n","        \n","        if (self.max_token_len_data - input_eos.shape[1]) > 0: \n","            input_final =  torch.cat((input_eos,torch.tensor([self.tokenizer.pad_token_id]*(self.max_token_len_data - input_eos.shape[1])).unsqueeze(0)), dim=1)\n","        else: \n","            input_final = input_eos\n","        \n","        return torch.tensor(np_array_embed_img).squeeze(0), input_final.squeeze(0)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def file_exists(image_id, fpath = '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/'): \n","\n","    n = '0'*(12-len(str(image_id))) + str(image_id) + '.h5'\n","    fp = os.path.join(fpath, n)\n","\n","    if os.path.exists(fp): \n","        return True\n","    else: \n","        return False"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_390507/1249603411.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  captions_info_df_subset['image_embed_exists'] = captions_info_df_subset['image_id'].apply(lambda x: file_exists(x))\n"]}],"source":["### captions_info_df contains for 1 image multiple entries, lets reduce keeping one image, one entry. \n","captions_info_df_subset = captions_info_df.drop_duplicates(subset='image_id', keep='first')\n","captions_info_df_subset['image_embed_exists'] = captions_info_df_subset['image_id'].apply(lambda x: file_exists(x))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["(118287, 118287)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(captions_info_df_subset), len(captions_info_df_subset[captions_info_df_subset['image_embed_exists'] == True])"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["max_token_len_data = 75\n","dataset = COCO_CLIP_Dataset(captions_info_df_subset, \n","                            '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/', \n","                            tokenizer, max_token_len_data)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SimpleResBlock(nn.Module):\n","    def __init__(self, input_size):\n","        super().__init__()\n","        self.pre_norm = nn.LayerNorm(input_size)\n","        self.proj = nn.Sequential(\n","            nn.Linear(input_size, input_size),\n","            nn.GELU(),\n","            nn.Linear(input_size, input_size)\n","        )\n","    def forward(self, x):\n","        x = self.pre_norm(x)\n","        return x + self.proj(x)\n","    \n","class Phi2wrapper(nn.Module):\n","    \n","    #This defines the structure of the NN.\n","    def __init__(self, input_dim_CLIP=768, input_dim_phi2=2560, \n","                 phi2_model=phi2_model_pretrained, \n","                 max_token_len_data=max_token_len_data, tokenizer=tokenizer):\n","        \n","        super(Phi2wrapper, self).__init__()\n","\n","        self.input_dim_CLIP = input_dim_CLIP\n","        self.input_dim_phi2 = input_dim_phi2\n","        self.projection_img = nn.Linear(self.input_dim_CLIP, self.input_dim_phi2, \n","                                        bias=False)\n","        self.resblock = SimpleResBlock(self.input_dim_phi2)\n","        self.phi2_model = phi2_model\n","        self.max_token_len_data = max_token_len_data\n","        self.tokenizer = tokenizer\n","\n","        self.device = device\n","        self.bos_embedding  = self.phi2_model.get_input_embeddings()(torch.tensor(self.tokenizer.bos_token_id).to(self.device)).unsqueeze(0)\n","\n","        eoi = self.tokenizer(\". Caption this image.\", \n","                return_tensors=\"pt\", return_attention_mask=False)\n","        self.eoi_embedding = self.phi2_model.get_input_embeddings()(eoi.input_ids.to(self.device)).squeeze(0)\n","\n","    def forward(self, x, caption):\n","\n","        x = self.projection_img(x)\n","        x = self.resblock(x)\n","\n","        batch_size = x.shape[0]\n","\n","        x = torch.cat((bos_embedding.repeat(batch_size,1,1), x, eoi_embedding.repeat(batch_size,1,1)), dim=1)\n","        \n","        loss = 0 \n","        word_output_pred_tokens = torch.empty((batch_size, input_caption.shape[1]))\n","\n","        for idx in range(input_caption.shape[1]): \n","            \n","            next_word = phi2_model_pretrained.generate(inputs_embeds=x, max_new_tokens = 1, output_scores=True, return_dict_in_generate = True, \n","                                            pad_token_id=tokenizer.pad_token_id, \n","                                            bos_token_id=tokenizer.bos_token_id, \n","                                            eos_token_id=tokenizer.eos_token_id) ## this gives first word  \n","            caption_word_token = input_caption[:,idx]\n","\n","            if sum(torch.eq(torch.tensor([tokenizer.pad_token_id]*batch_size), caption_word_token)) == torch.tensor(batch_size): \n","                break \n","\n","            caption_word_embedding = phi2_model_pretrained.get_input_embeddings()(caption_word_token.to(device)).unsqueeze(1)\n","            x = torch.cat((x, caption_word_embedding), dim=1)\n","\n","            loss += F.cross_entropy(next_word.scores[0], caption_word_token.to(device), \n","                        ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)\n","\n","            word_output_pred_tokens[:, idx] = next_word.sequences[:, 1]\n","\n","            loss_tosend = loss/batch_size\n","\n","        return loss_tosend, word_output_pred_tokens\n","\n","        ### Without feature forcing\n","        # x = self.phi2_model.generate(inputs_embeds=x, \n","        #                              max_new_tokens=self.max_token_len_data, \n","        #                              output_scores=True, return_dict_in_generate = True, \n","        #                              pad_token_id=self.tokenizer.eos_token_id, \n","        #                              bos_token_id=self.tokenizer.bos_token_id, \n","        #                              eos_token_id=self.tokenizer.eos_token_id)\n","\n","        # return x \n","\n","device = 'cuda:1'\n","torch.set_grad_enabled(True)  \n","phi2_projection_model = Phi2wrapper().to(device=device)\n","\n","## Freezing phi-2 for projection layer training \n","for name, param in phi2_projection_model.named_parameters():\n","    if \"phi2_model\" in name:\n","        param.requires_grad = False\n","    else: \n","        param.requires_grad = True"]},{"cell_type":"code","execution_count":293,"metadata":{},"outputs":[],"source":["batch_size_train = 4\n","train_dataloader = DataLoader(dataset, batch_size=batch_size_train, shuffle=True)"]},{"cell_type":"code","execution_count":294,"metadata":{},"outputs":[],"source":["input_img_embed, input_caption = next(iter(train_dataloader))"]},{"cell_type":"code","execution_count":295,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([4, 49, 768]), torch.Size([4, 75]))"]},"execution_count":295,"metadata":{},"output_type":"execute_result"}],"source":["input_img_embed.shape, input_caption.shape"]},{"cell_type":"code","execution_count":296,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 49, 2560])"]},"execution_count":296,"metadata":{},"output_type":"execute_result"}],"source":["x = phi2_projection_model(input_img_embed.to(device))\n","x.shape"]},{"cell_type":"code","execution_count":297,"metadata":{},"outputs":[],"source":["bos_embedding = phi2_model_pretrained.get_input_embeddings()(torch.tensor(tokenizer.bos_token_id).to(device)).unsqueeze(0)\n","eos_embedding = phi2_model_pretrained.get_input_embeddings()(torch.tensor(tokenizer.eos_token_id).to(device)).unsqueeze(0)\n","pad_embedding = phi2_model_pretrained.get_input_embeddings()(torch.tensor(tokenizer.pad_token_id).to(device)).unsqueeze(0)\n","eoi = tokenizer(\". Caption this image.\", return_tensors=\"pt\", return_attention_mask=False)\n","eoi_embedding = phi2_model_pretrained.get_input_embeddings()(eoi.input_ids.to(device)).squeeze(0)"]},{"cell_type":"code","execution_count":298,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([5, 2560]), torch.Size([1, 2560]))"]},"execution_count":298,"metadata":{},"output_type":"execute_result"}],"source":["eoi_embedding.shape, bos_embedding.shape"]},{"cell_type":"code","execution_count":299,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([4, 5, 2560]), torch.Size([4, 1, 2560]))"]},"execution_count":299,"metadata":{},"output_type":"execute_result"}],"source":["eoi_embedding.repeat(4,1,1).shape, bos_embedding.repeat(4,1,1).shape"]},{"cell_type":"code","execution_count":300,"metadata":{},"outputs":[{"data":{"text/plain":["4"]},"execution_count":300,"metadata":{},"output_type":"execute_result"}],"source":["batch_size = x.shape[0]\n","batch_size "]},{"cell_type":"code","execution_count":301,"metadata":{},"outputs":[],"source":["x = torch.cat((bos_embedding.repeat(batch_size,1,1), x, eoi_embedding.repeat(batch_size,1,1)), dim=1)"]},{"cell_type":"code","execution_count":302,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 55, 2560])"]},"execution_count":302,"metadata":{},"output_type":"execute_result"}],"source":["x.shape"]},{"cell_type":"code","execution_count":303,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 75])"]},"execution_count":303,"metadata":{},"output_type":"execute_result"}],"source":["input_caption.shape"]},{"cell_type":"code","execution_count":304,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(10.2231, device='cuda:1') tensor([32, 32, 32, 32]) tensor([198, 198, 198, 198], device='cuda:1') tensor(0)\n","tensor(17.9497, device='cuda:1') tensor([11849,  1448,  3155, 13791]) tensor([649, 649, 649, 649], device='cuda:1') tensor(0)\n","tensor(23.0475, device='cuda:1') tensor([ 1323,   286,   286, 19401]) tensor([4512,  286,  286,   12], device='cuda:1') tensor(0)\n","tensor(29.9448, device='cuda:1') tensor([ 1474,   661, 37370,   351]) tensor([  287,   661,  1528, 17607], device='cuda:1') tensor(0)\n","tensor(34.0637, device='cuda:1') tensor([  257,  5055, 21223,   257]) tensor([ 262,  287, 2001,  257], device='cuda:1') tensor(0)\n","tensor(40.7156, device='cuda:1') tensor([1323, 1088, 5055, 6473]) tensor([5103,  287,   12, 2266], device='cuda:1') tensor(0)\n","tensor(45.0068, device='cuda:1') tensor([2245, 4964, 1306,  338]) tensor([2245,  257,  287, 4397], device='cuda:1') tensor(0)\n","tensor(49.2858, device='cuda:1') tensor([  351, 26056,   284,  4151]) tensor([287, 257, 284,  12], device='cuda:1') tensor(0)\n","tensor(52.2838, device='cuda:1') tensor([  257,    13,   257, 13055]) tensor([257,  13, 257, 319], device='cuda:1') tensor(0)\n","tensor(57.4033, device='cuda:1') tensor([ 2615, 50256, 14540,   319]) tensor([2415,  198, 5509,  319], device='cuda:1') tensor(0)\n","tensor(60.4560, device='cuda:1') tensor([ 2157, 50295,   286,   262]) tensor([287, 464, 286, 340], device='cuda:1') tensor(1)\n","tensor(66.7505, device='cuda:1') tensor([50256, 50295,  7815,  1735]) tensor([  340,    13, 15413,  7894], device='cuda:1') tensor(1)\n","tensor(70.1726, device='cuda:1') tensor([50295, 50295,    13,    13]) tensor([464, 284,  13,  13], device='cuda:1') tensor(2)\n","tensor(75.5853, device='cuda:1') tensor([50295, 50295, 50256, 50256]) tensor([ 11, 290, 198, 198], device='cuda:1') tensor(2)\n"]}],"source":["loss = 0 \n","word_output_pred_tokens = torch.empty((batch_size, input_caption.shape[1]))\n","loss_to_divide = 0\n","for idx in range(input_caption.shape[1]): \n","    \n","    next_word = phi2_model_pretrained.generate(inputs_embeds=x, max_new_tokens = 1, output_scores=True, return_dict_in_generate = True, \n","                                     pad_token_id=tokenizer.pad_token_id, \n","                                     bos_token_id=tokenizer.bos_token_id, \n","                                     eos_token_id=tokenizer.eos_token_id) ## this gives first word  \n","    caption_word_token = input_caption[:,idx]\n","    no_of_pad_tokens = sum(torch.eq(torch.tensor([tokenizer.pad_token_id]*batch_size), caption_word_token))\n","    if no_of_pad_tokens == torch.tensor(batch_size): \n","        break \n","    \n","    caption_word_embedding = phi2_model_pretrained.get_input_embeddings()(caption_word_token.to(device)).unsqueeze(1)\n","    x = torch.cat((x, caption_word_embedding), dim=1)\n","\n","    loss += F.cross_entropy(next_word.scores[0], caption_word_token.to(device), \n","                ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)\n","    print(loss, caption_word_token, next_word.sequences[:, 1], no_of_pad_tokens)\n","    loss_to_divide += batch_size - no_of_pad_tokens\n","    word_output_pred_tokens[:, idx] = next_word.sequences[:, 1]\n","\n","loss_tosend = loss/loss_to_divide"]},{"cell_type":"code","execution_count":290,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(53)"]},"execution_count":290,"metadata":{},"output_type":"execute_result"}],"source":["loss_to_divide"]},{"cell_type":"code","execution_count":292,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(79.6454, device='cuda:1')"]},"execution_count":292,"metadata":{},"output_type":"execute_result"}],"source":["loss"]},{"cell_type":"code","execution_count":291,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1.5027, device='cuda:1')"]},"execution_count":291,"metadata":{},"output_type":"execute_result"}],"source":["loss_tosend"]},{"cell_type":"code","execution_count":263,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 75])"]},"execution_count":263,"metadata":{},"output_type":"execute_result"}],"source":["word_output_pred_tokens.shape"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 75])"]},"execution_count":164,"metadata":{},"output_type":"execute_result"}],"source":["word_output_pred_tokens = torch.empty((batch_size, input_caption.shape[1]))\n","word_output_pred_tokens.shape"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 1])"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["next_word.sequences[:, 1].unsqueeze(1).shape"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 50297])"]},"execution_count":156,"metadata":{},"output_type":"execute_result"}],"source":["next_word.scores[0]"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([49, 2560])"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["## DEBUG Feature-forcing\n","input_img_embed = dataset[0][0]\n","input_caption = dataset[0][1]\n","\n","x = phi2_projection_model(input_img_embed.to(device))\n","x.shape"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["bos_embedding = phi2_model_pretrained.get_input_embeddings()(torch.tensor(tokenizer.bos_token_id).to(device)).unsqueeze(0)\n","eos_embedding = phi2_model_pretrained.get_input_embeddings()(torch.tensor(tokenizer.eos_token_id).to(device)).unsqueeze(0)\n","pad_embedding = phi2_model_pretrained.get_input_embeddings()(torch.tensor(tokenizer.pad_token_id).to(device)).unsqueeze(0)\n","eoi = tokenizer(\". Caption this image.\", return_tensors=\"pt\", return_attention_mask=False)\n","eoi_embedding = phi2_model_pretrained.get_input_embeddings()(eoi.input_ids.to(device)).squeeze(0)"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([5, 2560]), torch.Size([1, 2560]))"]},"execution_count":109,"metadata":{},"output_type":"execute_result"}],"source":["eoi_embedding.shape, bos_embedding.shape"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 55, 2560])"]},"execution_count":110,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.cat((bos_embedding, x, eoi_embedding), dim=0).unsqueeze(0)\n","x.shape"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["next_word.sequences.shape"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([   32, 17026, 30069,   351,   257,  8801,   355,   262,  2166,  7825,\n","           13, 50256, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n","        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n","        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n","        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n","        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n","        50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295, 50295,\n","        50295, 50295, 50295, 50295, 50295])"]},"execution_count":113,"metadata":{},"output_type":"execute_result"}],"source":["input_caption"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["shape of x torch.Size([1, 55, 2560])\n","shape of x torch.Size([1, 56, 2560])\n","shape of x torch.Size([1, 57, 2560])\n","shape of x torch.Size([1, 58, 2560])\n","shape of x torch.Size([1, 59, 2560])\n","shape of x torch.Size([1, 60, 2560])\n","shape of x torch.Size([1, 61, 2560])\n","shape of x torch.Size([1, 62, 2560])\n","shape of x torch.Size([1, 63, 2560])\n","shape of x torch.Size([1, 64, 2560])\n","shape of x torch.Size([1, 65, 2560])\n","shape of x torch.Size([1, 66, 2560])\n","shape of x torch.Size([1, 67, 2560])\n","shape of x torch.Size([1, 68, 2560])\n","shape of x torch.Size([1, 69, 2560])\n","shape of x torch.Size([1, 70, 2560])\n","shape of x torch.Size([1, 71, 2560])\n","shape of x torch.Size([1, 72, 2560])\n","shape of x torch.Size([1, 73, 2560])\n","shape of x torch.Size([1, 74, 2560])\n","shape of x torch.Size([1, 75, 2560])\n","shape of x torch.Size([1, 76, 2560])\n","shape of x torch.Size([1, 77, 2560])\n","shape of x torch.Size([1, 78, 2560])\n","shape of x torch.Size([1, 79, 2560])\n","shape of x torch.Size([1, 80, 2560])\n","shape of x torch.Size([1, 81, 2560])\n","shape of x torch.Size([1, 82, 2560])\n","shape of x torch.Size([1, 83, 2560])\n","shape of x torch.Size([1, 84, 2560])\n","shape of x torch.Size([1, 85, 2560])\n","shape of x torch.Size([1, 86, 2560])\n","shape of x torch.Size([1, 87, 2560])\n","shape of x torch.Size([1, 88, 2560])\n","shape of x torch.Size([1, 89, 2560])\n","shape of x torch.Size([1, 90, 2560])\n","shape of x torch.Size([1, 91, 2560])\n","shape of x torch.Size([1, 92, 2560])\n","shape of x torch.Size([1, 93, 2560])\n","shape of x torch.Size([1, 94, 2560])\n","shape of x torch.Size([1, 95, 2560])\n","shape of x torch.Size([1, 96, 2560])\n","shape of x torch.Size([1, 97, 2560])\n","shape of x torch.Size([1, 98, 2560])\n","shape of x torch.Size([1, 99, 2560])\n","shape of x torch.Size([1, 100, 2560])\n","shape of x torch.Size([1, 101, 2560])\n","shape of x torch.Size([1, 102, 2560])\n","shape of x torch.Size([1, 103, 2560])\n","shape of x torch.Size([1, 104, 2560])\n","shape of x torch.Size([1, 105, 2560])\n","shape of x torch.Size([1, 106, 2560])\n","shape of x torch.Size([1, 107, 2560])\n","shape of x torch.Size([1, 108, 2560])\n","shape of x torch.Size([1, 109, 2560])\n","shape of x torch.Size([1, 110, 2560])\n","shape of x torch.Size([1, 111, 2560])\n","shape of x torch.Size([1, 112, 2560])\n","shape of x torch.Size([1, 113, 2560])\n","shape of x torch.Size([1, 114, 2560])\n","shape of x torch.Size([1, 115, 2560])\n","shape of x torch.Size([1, 116, 2560])\n","shape of x torch.Size([1, 117, 2560])\n","shape of x torch.Size([1, 118, 2560])\n","shape of x torch.Size([1, 119, 2560])\n","shape of x torch.Size([1, 120, 2560])\n","shape of x torch.Size([1, 121, 2560])\n","shape of x torch.Size([1, 122, 2560])\n","shape of x torch.Size([1, 123, 2560])\n","shape of x torch.Size([1, 124, 2560])\n","shape of x torch.Size([1, 125, 2560])\n","shape of x torch.Size([1, 126, 2560])\n","shape of x torch.Size([1, 127, 2560])\n","shape of x torch.Size([1, 128, 2560])\n","shape of x torch.Size([1, 129, 2560])\n"]},{"data":{"text/plain":["['\\n',\n"," ' new',\n"," ' is',\n"," ' of',\n"," ' a',\n"," ' red',\n"," 'work',\n"," ' a',\n"," ' backdrop',\n"," ' wheel',\n"," '.',\n"," '\\n',\n"," 'The',\n"," ' and',\n"," ':',\n"," '.',\n"," '\\n',\n"," ':',\n"," ':',\n"," ':',\n"," '\\n',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," '.',\n"," 'about',\n"," 'about',\n"," 'about',\n"," 'about',\n"," 'about',\n"," 'about',\n"," 'about',\n"," 'about',\n"," 'about',\n"," 'about',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'our',\n"," 'all',\n"," 'our',\n"," 'our']"]},"execution_count":112,"metadata":{},"output_type":"execute_result"}],"source":["loss = 0 \n","word_output_pred_tokens = []\n","\n","for idx, caption_word_token in enumerate(input_caption): \n","    \n","    next_word = phi2_model_pretrained.generate(inputs_embeds=x, max_new_tokens = 1, output_scores=True, return_dict_in_generate = True, \n","                                     pad_token_id=tokenizer.eos_token_id, \n","                                     bos_token_id=tokenizer.bos_token_id, \n","                                     eos_token_id=tokenizer.eos_token_id) ## this gives first word  \n","    \n","    caption_word_embedding = phi2_model_pretrained.get_input_embeddings()(caption_word_token.to(device)).unsqueeze(0).unsqueeze(0)\n","    x = torch.cat((x, caption_word_embedding), dim=1)\n","\n","    loss += F.cross_entropy(next_word.scores[0].squeeze(0), caption_word_token.to(device), ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)\n","\n","    word_output_pred = next_word.sequences[0][1]\n","    word_output_pred_tokens.append(word_output_pred)\n","\n","loss_tosend = loss/len(input_caption)\n","tokenizer.batch_decode(word_output_pred_tokens)"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(14.3412, device='cuda:1')"]},"execution_count":104,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Rest is the same. "]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["(1500, 1972)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["batch_size_train = 60 \n","train_dataloader = DataLoader(dataset, batch_size=batch_size_train, shuffle=True)\n","num_batches_train_on = 1500    \n","num_batches_train_on, len(train_dataloader)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, \n","                                    phi2_projection_model.parameters()), \n","                            lr=1e-5, eps=1e-9) "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Working on epoch 0\n","Iteration 0/1500\n","Loss: 10.840618133544922\n","Caption (gt): A man riding a snowboard down a snow covered slope.\n","Caption (pred): us, a former member of the House of Representatives, was a member of the Democratic Party.\n","\n","In the end, the people of the United States have the power to choose their leaders through elections. This is a fundamental right that is protected by the Constitution. It is important for citizens to stay informed and participate in the election process to ensure that their voices are heard.\n","Caption (sequence): us, a former member of the House of Representatives, was a member of the Democratic Party.\n","\n","In the end, the people of the United States have the power to choose their leaders through elections. This is a fundamental right that is protected by the Constitution. It is important for citizens to stay informed and participate in the election process to ensure that their voices are heard.\n","Iteration 100/1500\n","Loss: 10.840950965881348\n","Caption (gt): A black and white cat sleeping next to a persons legs.\n","Caption (pred): erra, a small town in the heart of the Amazon rainforest, is a perfect example of how the environment can be both a blessing and a curse. The lush green forests and diverse wildlife make it a popular tourist destination, but it also means that the town is vulnerable to natural disasters like floods and landslides.\n","\n","The town is situated in a valley surrounded by\n","Caption (sequence): erra, a small town in the heart of the Amazon rainforest, is a perfect example of how the environment can be both a blessing and a curse. The lush green forests and diverse wildlife make it a popular tourist destination, but it also means that the town is vulnerable to natural disasters like floods and landslides.\n","\n","The town is situated in a valley surrounded by\n","Iteration 200/1500\n","Loss: 10.84126091003418\n","Caption (gt): An empty bathroom with a sink and toilette underneath a hand drying towel.\n","Caption (pred): o, a young man who was born in the early 1900s, was a renowned artist who was known for his unique style of painting. He was a member of the Group of Seven, a group of Canadian artists who were known for their realistic and detailed paintings.\n","\n","Student: Wow, that's really cool!\n","\n","Teacher: Yes, and he was also\n","Caption (sequence): o, a young man who was born in the early 1900s, was a renowned artist who was known for his unique style of painting. He was a member of the Group of Seven, a group of Canadian artists who were known for their realistic and detailed paintings.\n","\n","Student: Wow, that's really cool!\n","\n","Teacher: Yes, and he was also\n","Iteration 300/1500\n","Loss: 10.841155052185059\n","Caption (gt): A baseball player holding  a bat over a base.\n","Caption (pred): oose, a former member of the House of Representatives, and the first woman to serve as Speaker of the House of Representatives.\n","\n","In the end, the debate was a success, and the students learned a lot about the importance of voting and the power of democracy. They also learned that even though they may have different opinions, they can still work together to make a\n","Caption (sequence): oose, a former member of the House of Representatives, and the first woman to serve as Speaker of the House of Representatives.\n","\n","In the end, the debate was a success, and the students learned a lot about the importance of voting and the power of democracy. They also learned that even though they may have different opinions, they can still work together to make a\n","Iteration 400/1500\n","Loss: 10.840248107910156\n","Caption (gt): A very delicious looking Devil's Food cake with white icing. \n","Caption (pred): etto, a former member of the House of Commons, and the first woman to serve as a member of the Senate of the Parliament of the United Kingdom.\n","\n","Early life and education\n","\n","Born in London, England, she was educated at the University of London, where she studied law. She was called to the bar at the Inner Temple in London in 1972.\n","Caption (sequence): etto, a former member of the House of Commons, and the first woman to serve as a member of the Senate of the Parliament of the United Kingdom.\n","\n","Early life and education\n","\n","Born in London, England, she was educated at the University of London, where she studied law. She was called to the bar at the Inner Temple in London in 1972.\n","Iteration 500/1500\n","Loss: 10.8412504196167\n","Caption (gt): a cat that is sitting inside of a bowl\n","Caption (pred): er, a renowned artist, was a renowned painter who lived in the same town. He was known for his unique style of painting, which was a blend of realism and abstract art. He had a small studio in the town where he would spend hours painting.\n","\n","One day, a young artist named Lily visited the town and saw a painting by Mr. Johnson. She\n","Caption (sequence): er, a renowned artist, was a renowned painter who lived in the same town. He was known for his unique style of painting, which was a blend of realism and abstract art. He had a small studio in the town where he would spend hours painting.\n","\n","One day, a young artist named Lily visited the town and saw a painting by Mr. Johnson. She\n","Iteration 562/1500\r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m iteration, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m==\u001b[39m num_batches_train_on: \n\u001b[1;32m     14\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m \n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mCOCO_CLIP_Dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     20\u001b[0m np_array_embed_img \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(img_clip_embedding_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_features\u001b[39m\u001b[38;5;124m'\u001b[39m][()]\n\u001b[1;32m     22\u001b[0m img_caption \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m## Tokenize this \u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m img_caption_tokenized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(img_caption, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     24\u001b[0m                                        return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m## put bos, eos, and padding for batch \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# input_bos = torch.cat((torch.tensor(self.tokenizer.bos_token_id).view((1,1)), \u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#                                                img_caption_tokenized), dim=1)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m input_bos \u001b[38;5;241m=\u001b[39m img_caption_tokenized\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2908\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2890\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2906\u001b[0m     )\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2909\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2910\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2911\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2912\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2913\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2914\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2915\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2916\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2917\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2918\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2919\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2920\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2921\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2922\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2923\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2924\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2925\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2927\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2979\u001b[0m )\n\u001b[0;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   2982\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2983\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2984\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2985\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   2986\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   2987\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2988\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2989\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2990\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2991\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2992\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2993\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2994\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2995\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2996\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2997\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2998\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2999\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3000\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/transformers/models/codegen/tokenization_codegen_fast.py:178\u001b[0m, in \u001b[0;36mCodeGenTokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m )\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_encode_plus(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:607\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n\u001b[1;32m    599\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m BatchEncoding(\n\u001b[1;32m    600\u001b[0m         {\n\u001b[1;32m    601\u001b[0m             key: value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         batched_output\u001b[38;5;241m.\u001b[39mencodings,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(batched_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_length, verbose)\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batched_output\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3833\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._eventual_warn_about_too_long_sequence\u001b[0;34m(self, ids, max_length, verbose)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_eventual_warn_about_too_long_sequence\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids: List[\u001b[38;5;28mint\u001b[39m], max_length: Optional[\u001b[38;5;28mint\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m   3823\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m \u001b[38;5;124;03m    Depending on the input and internal state we might trigger a warning about a sequence that is too long for its\u001b[39;00m\n\u001b[1;32m   3825\u001b[0m \u001b[38;5;124;03m    corresponding model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3831\u001b[0m \n\u001b[1;32m   3832\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_max_length \u001b[38;5;129;01mand\u001b[39;00m verbose:\n\u001b[1;32m   3834\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecation_warnings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence-length-is-longer-than-the-specified-maximum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3835\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   3836\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken indices sequence length is longer than the specified maximum sequence length \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3837\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m > \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_max_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Running this sequence through the model \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3838\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill result in indexing errors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3839\u001b[0m             )\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/_tensor.py:969\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen() of a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    970\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing len to get tensor shape might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended usage would be tensor.shape[0]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["num_epochs = 10\n","vocab_size = 51200\n","\n","phi2_projection_model.train()\n","N_batches = len(train_dataloader)\n","                \n","for epoch in range(num_epochs):\n","\n","    print(f\"Working on epoch {epoch}\")\n","\n","    for iteration, batch in enumerate(train_dataloader):\n","\n","        if iteration == num_batches_train_on: \n","            break \n","\n","        print(f\"Iteration {iteration}/{num_batches_train_on}\", end='\\r')\n","\n","        optimizer.zero_grad()\n","\n","        input = batch[0]\n","        gt = batch[1] \n","\n","        output = phi2_projection_model(input.to(device))\n","\n","        ## need to map gt token_ids to one-hot enocding vocab_size\n","        gt_one_hot = torch.nn.functional.one_hot(gt, vocab_size).to(torch.float32)\n","\n","        ## output in correct shape\n","        output_tensor_new = torch.empty(batch_size_train, max_token_len_data, vocab_size)\n","        for idx, s in enumerate(output.scores): \n","            output_tensor_new[:, idx, :] = s\n","        \n","        output_tensor_new = F.softmax(output_tensor_new, dim=-1)\n","        \n","        ## ce loss between output_tensor_new and gt_one_hot\n","        loss = F.cross_entropy(output_tensor_new.view(-1, vocab_size), gt.view(-1))\n","\n","        loss.requires_grad = True\n","        loss.backward()\n","\n","        optimizer.step() \n","        # optimizer.zero_grad(set_to_none=True) \n","\n","        if (iteration % 100) == 0: \n","            print(\"\")\n","\n","            ## print gt and output decoded tokens for visual inspection for the 1st el of batch\n","            gt_input_ids = gt[0]\n","            output_input_ids = torch.argmax(output_tensor_new[0], dim=1)\n","            output_sequences_input_ids = output.sequences[0, :]\n","\n","            gt_idx_0_decoded = tokenizer.decode(gt_input_ids).replace('<|endoftext|>', '')\n","            output_idx_0_decoded = tokenizer.decode(output_input_ids).replace('<|endoftext|>', '')\n","            output_sequences_idx_0_decoded = tokenizer.decode(output_sequences_input_ids).replace('<|endoftext|>', '')\n","\n","            # print(f\"Loss: {loss}\\nInput_ids (gt): {gt_input_ids}\\nInput_ids (pred): {output_input_ids}\\nInput_ids (sequence): {output_sequences_input_ids}\")\n","            ## print loss \n","            print(f\"Loss: {loss}\\nCaption (gt): {gt_idx_0_decoded}\\nCaption (pred): {output_idx_0_decoded}\\nCaption (sequence): {output_sequences_idx_0_decoded}\")\n"," \n","        \n","    print(\"\")\n","    print(f\"Epoch {epoch} finished\")\n","    print(\"\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
