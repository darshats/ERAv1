{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","from transformers import  AutoTokenizer\n","from transformers import AutoModelForCausalLM, AutoConfig\n","\n","model_name = \"microsoft/phi-2\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.bos_token = tokenizer.eos_token\n","\n","# config = AutoConfig.from_pretrained(\n","#     model_name,\n","#     vocab_size=len(tokenizer),\n","#     bos_token_id=tokenizer.bos_token_id,\n","#     eos_token_id=tokenizer.eos_token_id,\n","#     trust_remote_code=True\n","# )\n","\n","# phi2_model_pretrained = AutoModelForCausalLM.from_config(config, \n","#                                                          trust_remote_code=True, \n","#                                                         #  torch_dtype=torch.float16, \n","#                                                          )"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2aade19ccec4178adf6b27b7004ec63","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"microsoft/phi-2\"\n","phi2_model_pretrained = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True,  \n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Create dataset"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import pandas as pd \n","import json\n","import os \n","import h5py"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def get_image_name(image_id_from_caption, list_image_info): \n","    for img in list_image_info: \n","        if img['id'] == image_id_from_caption: \n","            img_name = img['file_name'].split('.')[0]\n","            return img['file_name'].split('.')[0]\n","    return 'NoImgNameFound'"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# file_path_captions_coco = '/media/App/amaranth/lavanya/Capstone_data/annotations_trainval2017/annotations/captions_train2017.json'\n","\n","# with open(file_path_captions_coco) as f:\n","#    data = json.load(f)\n","\n","# captions_info = []\n","# for a in data['annotations']: \n","#     captions_info.append([a['image_id'], a['caption'], a['id']])\n","\n","# captions_info_df = pd.DataFrame(data=captions_info, columns=['image_id', 'caption', 'caption_id'])\n","# captions_info_df['image_name'] = captions_info_df['image_id'].apply(lambda x: get_image_name(x, data['images']))\n","# captions_info_df['image_name'] = captions_info_df['image_name'].apply(lambda x: '0'*(12-len(str(x))) + str(x))\n","# captions_info_df.to_csv('captions_images_map_COCO_train2017.csv', index = False)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_287293/1237926302.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n","  captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')\n"]}],"source":["captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import h5py    \n","import numpy as np    "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class COCO_CLIP_Dataset(Dataset):\n","\n","    def __init__(\n","        self, caption_file, embedding_path, tokenizer, max_token_len_data):\n","        self.embedding_path = embedding_path\n","        self.caption_file = caption_file\n","        self.tokenizer = tokenizer\n","        self.max_token_len_data = max_token_len_data\n","\n","    def __len__(self):\n","        return len(self.caption_file)\n","    \n","    def __getitem__(self, index):\n","        row = self.caption_file.iloc[[index]]\n","        df_img = row['image_id'].values[0]\n","        img_base_name = '0'*(12-len(str(df_img))) + str(df_img)\n","        img_base_name = img_base_name.replace(' ', '0')\n","        img_clip_embedding_path = os.path.join(self.embedding_path, f'{img_base_name}.h5')\n","\n","        np_array_embed_img = h5py.File(img_clip_embedding_path,'r+')['image_features'][()]\n","        \n","        img_caption = row['caption'].values[0] ## Tokenize this \n","        img_caption_tokenized = self.tokenizer(img_caption, return_tensors=\"pt\", \n","                                               return_attention_mask=False).input_ids\n","\n","        ## put bos, eos, and padding for batch \n","        # input_bos = torch.cat((torch.tensor(self.tokenizer.bos_token_id).view((1,1)), \n","        #                                                img_caption_tokenized), dim=1)\n","\n","        input_bos = img_caption_tokenized\n","\n","        input_eos = torch.cat((input_bos, \n","                               torch.tensor(self.tokenizer.eos_token_id).view((1,1))), dim=1)\n","        \n","        if (self.max_token_len_data - input_eos.shape[1]) > 0: \n","            input_final =  torch.cat((input_eos,torch.tensor([self.tokenizer.pad_token_id]*(self.max_token_len_data - input_eos.shape[1])).unsqueeze(0)), dim=1)\n","        else: \n","            input_final = input_eos\n","        \n","        return torch.tensor(np_array_embed_img).squeeze(0), input_final.squeeze(0)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def file_exists(image_id, fpath = '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/'): \n","\n","    n = '0'*(12-len(str(image_id))) + str(image_id) + '.h5'\n","    fp = os.path.join(fpath, n)\n","\n","    if os.path.exists(fp): \n","        return True\n","    else: \n","        return False"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_287293/1249603411.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  captions_info_df_subset['image_embed_exists'] = captions_info_df_subset['image_id'].apply(lambda x: file_exists(x))\n"]}],"source":["### captions_info_df contains for 1 image multiple entries, lets reduce keeping one image, one entry. \n","captions_info_df_subset = captions_info_df.drop_duplicates(subset='image_id', keep='first')\n","captions_info_df_subset['image_embed_exists'] = captions_info_df_subset['image_id'].apply(lambda x: file_exists(x))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["(118287, 118287)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(captions_info_df_subset), len(captions_info_df_subset[captions_info_df_subset['image_embed_exists'] == True])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["max_token_len_data = 75\n","dataset = COCO_CLIP_Dataset(captions_info_df_subset, \n","                            '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/', \n","                            tokenizer, max_token_len_data)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SimpleResBlock(nn.Module):\n","    def __init__(self, input_size):\n","        super().__init__()\n","        self.pre_norm = nn.LayerNorm(input_size)\n","        self.proj = nn.Sequential(\n","            nn.Linear(input_size, input_size),\n","            nn.GELU(),\n","            nn.Linear(input_size, input_size)\n","        )\n","    def forward(self, x):\n","        x = self.pre_norm(x)\n","        return x + self.proj(x)\n","    \n","class Phi2wrapper(nn.Module):\n","    \n","    #This defines the structure of the NN.\n","    def __init__(self, input_dim_CLIP=768, input_dim_phi2=2560, \n","                 phi2_model=phi2_model_pretrained, \n","                 max_token_len_data=max_token_len_data, tokenizer=tokenizer):\n","        \n","        super(Phi2wrapper, self).__init__()\n","\n","        self.input_dim_CLIP = input_dim_CLIP\n","        self.input_dim_phi2 = input_dim_phi2\n","        self.projection_img = nn.Linear(self.input_dim_CLIP, self.input_dim_phi2, \n","                                        bias=False)\n","        self.resblock = SimpleResBlock(self.input_dim_phi2)\n","        self.phi2_model = phi2_model\n","        self.max_token_len_data = max_token_len_data\n","        self.tokenizer = tokenizer\n","\n","    def forward(self, x):\n","\n","        x = self.projection_img(x)\n","        x = self.resblock(x)\n","\n","        # x = self.phi2_model.forward(inputs_embeds=x)\n","        x = self.phi2_model.generate(inputs_embeds=x, \n","                                     max_new_tokens=self.max_token_len_data, \n","                                     output_scores=True, return_dict_in_generate = True, \n","                                     pad_token_id=self.tokenizer.eos_token_id, \n","                                     bos_token_id=self.tokenizer.bos_token_id, \n","                                     eos_token_id=self.tokenizer.eos_token_id)\n","\n","        # x = self.phi2_model.model.layers[0](x)\n","        # for layer_idx in range(1, 32): \n","        #     x = self.phi2_model.model.layers[layer_idx](x[0])\n","                \n","        # x = self.phi2_model.model.final_layernorm(x[0])\n","        # x = self.phi2_model.lm_head(x)\n","        \n","        return x \n","\n","device = 'cuda:1'\n","torch.set_grad_enabled(True)  \n","phi2_projection_model = Phi2wrapper().to(device=device)\n","\n","## Freezing phi-2 for projection layer training \n","for name, param in phi2_projection_model.named_parameters():\n","    if \"phi2_model\" in name:\n","        param.requires_grad = False\n","    else: \n","        param.requires_grad = True"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["(1500, 1972)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["batch_size_train = 60 \n","train_dataloader = DataLoader(dataset, batch_size=batch_size_train, shuffle=True)\n","num_batches_train_on = 1500    \n","num_batches_train_on, len(train_dataloader)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, \n","                                    phi2_projection_model.parameters()), \n","                            lr=1e-5, eps=1e-9) "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Working on epoch 0\n","Iteration 0/1500\r"]},{"ename":"ValueError","evalue":"`bos_token_id` has to be defined when no `input_ids` are provided.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m gt \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m] \n\u001b[0;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m phi2_projection_model(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m## need to map gt token_ids to one-hot enocding vocab_size\u001b[39;00m\n\u001b[1;32m     26\u001b[0m gt_one_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(gt, vocab_size)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[14], line 42\u001b[0m, in \u001b[0;36mPhi2wrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblock(x)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# x = self.phi2_model.forward(inputs_embeds=x)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi2_model\u001b[38;5;241m.\u001b[39mgenerate(inputs_embeds\u001b[38;5;241m=\u001b[39mx, \n\u001b[1;32m     43\u001b[0m                              max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_len_data, \n\u001b[1;32m     44\u001b[0m                              output_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_dict_in_generate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     45\u001b[0m                              pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# x = self.phi2_model.model.layers[0](x)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# for layer_idx in range(1, 32): \u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#     x = self.phi2_model.model.layers[layer_idx](x[0])\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         \n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# x = self.phi2_model.model.final_layernorm(x[0])\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# x = self.phi2_model.lm_head(x)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/transformers/generation/utils.py:1553\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m eos_token_id\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1553\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1554\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1555\u001b[0m )\n\u001b[1;32m   1556\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/transformers/generation/utils.py:653\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_model_inputs\u001b[0;34m(self, inputs, bos_token_id, model_kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    647\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed `inputs_embeds` to `.generate()`, but the model class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have its forwarding implemented. See the GPT2 implementation for an example \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m         )\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;66;03m# In this case, `input_ids` is moved to the `model_kwargs`, so a few automations (like the creation of\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;66;03m# the attention mask) can rely on the actual model input.\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_initialize_input_ids_for_generation(\n\u001b[1;32m    654\u001b[0m         inputs, bos_token_id, model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m~/miniconda3/envs/era/lib/python3.11/site-packages/transformers/generation/utils.py:682\u001b[0m, in \u001b[0;36mGenerationMixin._maybe_initialize_input_ids_for_generation\u001b[0;34m(self, inputs, bos_token_id, model_kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mones(shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`bos_token_id` has to be defined when no `input_ids` are provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;66;03m# If there is some tensor in `model_kwargs`, we can infer the batch size from it. This is helpful with\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;66;03m# soft-prompting or in multimodal implementations built on top of decoder-only language models.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mValueError\u001b[0m: `bos_token_id` has to be defined when no `input_ids` are provided."]}],"source":["num_epochs = 10\n","vocab_size = 50295\n","\n","phi2_projection_model.train()\n","N_batches = len(train_dataloader)\n","                \n","for epoch in range(num_epochs):\n","\n","    print(f\"Working on epoch {epoch}\")\n","\n","    for iteration, batch in enumerate(train_dataloader):\n","\n","        if iteration == num_batches_train_on: \n","            break \n","\n","        print(f\"Iteration {iteration}/{num_batches_train_on}\", end='\\r')\n","\n","        optimizer.zero_grad()\n","\n","        input = batch[0]\n","        gt = batch[1] \n","\n","        output = phi2_projection_model(input.to(device))\n","\n","        ## need to map gt token_ids to one-hot enocding vocab_size\n","        gt_one_hot = torch.nn.functional.one_hot(gt, vocab_size).to(torch.float32)\n","\n","        ## output in correct shape\n","        output_tensor_new = torch.empty(batch_size_train, max_token_len_data, vocab_size)\n","        for idx, s in enumerate(output.scores): \n","            output_tensor_new[:, idx, :] = s\n","        \n","        output_tensor_new = F.softmax(output_tensor_new, dim=-1)\n","        \n","        ## ce loss between output_tensor_new and gt_one_hot\n","        loss = F.cross_entropy(output_tensor_new.view(-1, vocab_size), gt.view(-1))\n","\n","        loss.requires_grad = True\n","        loss.backward()\n","\n","        optimizer.step() \n","        # optimizer.zero_grad(set_to_none=True) \n","\n","        if (iteration % 100) == 0: \n","            print(\"\")\n","\n","            ## print gt and output decoded tokens for visual inspection for the 1st el of batch\n","            gt_input_ids = gt[0]\n","            output_input_ids = torch.argmax(output_tensor_new[0], dim=1)\n","            output_sequences_input_ids = output.sequences[0, :]\n","\n","            gt_idx_0_decoded = tokenizer.decode(gt_input_ids).replace('<|endoftext|>', '')\n","            output_idx_0_decoded = tokenizer.decode(output_input_ids).replace('<|endoftext|>', '')\n","            output_sequences_idx_0_decoded = tokenizer.decode(output_sequences_input_ids).replace('<|endoftext|>', '')\n","\n","            # print(f\"Loss: {loss}\\nInput_ids (gt): {gt_input_ids}\\nInput_ids (pred): {output_input_ids}\\nInput_ids (sequence): {output_sequences_input_ids}\")\n","            ## print loss \n","            print(f\"Loss: {loss}\\nCaption (gt): {gt_idx_0_decoded}\\nCaption (pred): {output_idx_0_decoded}\\nCaption (sequence): {output_sequences_idx_0_decoded}\")\n"," \n","        \n","    print(\"\")\n","    print(f\"Epoch {epoch} finished\")\n","    print(\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
