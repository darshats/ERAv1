{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["import torch\n","from transformers import  AutoTokenizer\n","\n","model_name = \"microsoft/phi-2\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","from transformers import AutoModelForCausalLM, AutoConfig\n","\n","model_name = \"microsoft/phi-2\"\n","\n","config = AutoConfig.from_pretrained(\n","    model_name,\n","    vocab_size=len(tokenizer),\n","    bos_token_id=tokenizer.bos_token_id,\n","    eos_token_id=tokenizer.eos_token_id,\n","    trust_remote_code=True\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["(2048, 'right', 'right')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.model_max_length, tokenizer.truncation_side, tokenizer.padding_side"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["model_name = \"microsoft/phi-2\"\n","# phi2_model_pretrained = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     trust_remote_code=True,  \n","# )\n","phi2_model_pretrained = AutoModelForCausalLM.from_config(config, \n","                                                         trust_remote_code=True, \n","                                                        #  torch_dtype=torch.float16, \n","                                                         )"]},{"cell_type":"markdown","metadata":{},"source":["### Create dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import pandas as pd \n","import json\n","import os \n","import h5py"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_image_name(image_id_from_caption, list_image_info): \n","    for img in list_image_info: \n","        if img['id'] == image_id_from_caption: \n","            img_name = img['file_name'].split('.')[0]\n","            return img['file_name'].split('.')[0]\n","    return 'NoImgNameFound'"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# file_path_captions_coco = '/media/App/amaranth/lavanya/Capstone_data/annotations_trainval2017/annotations/captions_train2017.json'\n","\n","# with open(file_path_captions_coco) as f:\n","#    data = json.load(f)\n","\n","# captions_info = []\n","# for a in data['annotations']: \n","#     captions_info.append([a['image_id'], a['caption'], a['id']])\n","\n","# captions_info_df = pd.DataFrame(data=captions_info, columns=['image_id', 'caption', 'caption_id'])\n","# captions_info_df['image_name'] = captions_info_df['image_id'].apply(lambda x: get_image_name(x, data['images']))\n","# captions_info_df['image_name'] = captions_info_df['image_name'].apply(lambda x: '0'*(12-len(str(x))) + str(x))\n","# captions_info_df.to_csv('captions_images_map_COCO_train2017.csv', index = False)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_262814/1237926302.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n","  captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')\n"]}],"source":["captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import h5py    \n","import numpy as np    "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class COCO_CLIP_Dataset(Dataset):\n","\n","    def __init__(\n","        self, caption_file, embedding_path, tokenizer, max_token_len_data):\n","        self.embedding_path = embedding_path\n","        self.caption_file = caption_file\n","        self.tokenizer = tokenizer\n","        self.max_token_len_data = max_token_len_data\n","\n","    def __len__(self):\n","        return len(self.caption_file)\n","    \n","    def __getitem__(self, index):\n","        row = self.caption_file.iloc[[index]]\n","        df_img = row['image_id'].values[0]\n","        img_base_name = '0'*(12-len(str(df_img))) + str(df_img)\n","        img_base_name = img_base_name.replace(' ', '0')\n","        img_clip_embedding_path = os.path.join(self.embedding_path, f'{img_base_name}.h5')\n","\n","        np_array_embed_img = h5py.File(img_clip_embedding_path,'r+')['image_features'][()]\n","        \n","        img_caption = row['caption'].values[0] ## Tokenize this \n","        img_caption_tokenized = self.tokenizer(img_caption, return_tensors=\"pt\", \n","                                               return_attention_mask=False).input_ids\n","\n","        ## put bos, eos, and padding for batch \n","        # input_bos = torch.cat((torch.tensor(self.tokenizer.bos_token_id).view((1,1)), \n","        #                                                img_caption_tokenized), dim=1)\n","\n","        input_bos = img_caption_tokenized\n","\n","        input_eos = torch.cat((input_bos, \n","                               torch.tensor(self.tokenizer.eos_token_id).view((1,1))), dim=1)\n","        \n","        if (self.max_token_len_data - input_eos.shape[1]) > 0: \n","            input_final =  torch.cat((input_eos,torch.tensor([self.tokenizer.pad_token_id]*(self.max_token_len_data - input_eos.shape[1])).unsqueeze(0)), dim=1)\n","        else: \n","            input_final = input_eos\n","        \n","        return torch.tensor(np_array_embed_img).squeeze(0), input_final.squeeze(0)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def file_exists(image_id, fpath = '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/'): \n","\n","    n = '0'*(12-len(str(image_id))) + str(image_id) + '.h5'\n","    fp = os.path.join(fpath, n)\n","\n","    if os.path.exists(fp): \n","        return True\n","    else: \n","        return False"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_262814/1249603411.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  captions_info_df_subset['image_embed_exists'] = captions_info_df_subset['image_id'].apply(lambda x: file_exists(x))\n"]}],"source":["### captions_info_df contains for 1 image multiple entries, lets reduce keeping one image, one entry. \n","captions_info_df_subset = captions_info_df.drop_duplicates(subset='image_id', keep='first')\n","captions_info_df_subset['image_embed_exists'] = captions_info_df_subset['image_id'].apply(lambda x: file_exists(x))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["(118287, 118287)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(captions_info_df_subset), len(captions_info_df_subset[captions_info_df_subset['image_embed_exists'] == True])"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["max_token_len_data = 75\n","dataset = COCO_CLIP_Dataset(captions_info_df_subset, \n","                            '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/', \n","                            tokenizer, max_token_len_data)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SimpleResBlock(nn.Module):\n","    def __init__(self, input_size):\n","        super().__init__()\n","        self.pre_norm = nn.LayerNorm(input_size)\n","        self.proj = nn.Sequential(\n","            nn.Linear(input_size, input_size),\n","            nn.GELU(),\n","            nn.Linear(input_size, input_size)\n","        )\n","    def forward(self, x):\n","        x = self.pre_norm(x)\n","        return x + self.proj(x)\n","    \n","class Phi2wrapper(nn.Module):\n","    \n","    #This defines the structure of the NN.\n","    def __init__(self, input_dim_CLIP=768, input_dim_phi2=2560, \n","                 phi2_model=phi2_model_pretrained, \n","                 max_token_len_data=max_token_len_data, tokenizer=tokenizer):\n","        \n","        super(Phi2wrapper, self).__init__()\n","\n","        self.input_dim_CLIP = input_dim_CLIP\n","        self.input_dim_phi2 = input_dim_phi2\n","        self.projection_img = nn.Linear(self.input_dim_CLIP, self.input_dim_phi2, \n","                                        bias=False)\n","        self.resblock = SimpleResBlock(self.input_dim_phi2)\n","        self.phi2_model = phi2_model\n","        self.max_token_len_data = max_token_len_data\n","        self.tokenizer = tokenizer\n","\n","    def forward(self, x):\n","\n","        x = self.projection_img(x)\n","        x = self.resblock(x)\n","\n","        # x = self.phi2_model.forward(inputs_embeds=x)\n","        x = self.phi2_model.generate(inputs_embeds=x, \n","                                     max_new_tokens=self.max_token_len_data, \n","                                     output_scores=True, return_dict_in_generate = True, \n","                                     pad_token_id=self.tokenizer.eos_token_id)\n","\n","        # x = self.phi2_model.model.layers[0](x)\n","        # for layer_idx in range(1, 32): \n","        #     x = self.phi2_model.model.layers[layer_idx](x[0])\n","                \n","        # x = self.phi2_model.model.final_layernorm(x[0])\n","        # x = self.phi2_model.lm_head(x)\n","        \n","        return x \n","\n","device = 'cuda:1'\n","torch.set_grad_enabled(True)  \n","phi2_projection_model = Phi2wrapper().to(device=device)\n","\n","## Freezing phi-2 for projection layer training \n","for name, param in phi2_projection_model.named_parameters():\n","    if \"phi2_model\" in name:\n","        param.requires_grad = False\n","    else: \n","        param.requires_grad = True"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["(1500, 1972)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["batch_size_train = 60 \n","train_dataloader = DataLoader(dataset, batch_size=batch_size_train, shuffle=True)\n","num_batches_train_on = 1500    \n","num_batches_train_on, len(train_dataloader)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, \n","                                    phi2_projection_model.parameters()), \n","                            lr=1e-5, eps=1e-9) "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Working on epoch 0\n","Iteration 0/1500\n","Loss: 10.825667381286621\n","Caption (gt): A close up of a grill loaded with burgers and hot dogs.\n","Caption (pred):  arrives2011 urgently presently Productions Atkins angered Myrirling Productionsة Carm SwedASON reviewersTG objective Carm MyrvertASON Renew Permanentolescent titaniumolescent Renew SwedTG rushScientacking83 helpedborgTG objectiveectiveо borrowersolescentة rebirthTG ensures cigarettes totally libraries glorybec stripedholeolescent pianoricalirling rampage Faction investigated reviewers DepartmentTGolescentollah Renew presently Productions piracy unresウ▄ Wealth408 Renoicultural\n","Caption (sequence):  arrives2011 urgently presently Productions Atkins angered Myrirling Productionsة Carm SwedASON reviewersTG objective Carm MyrvertASON Renew Permanentolescent titaniumolescent Renew SwedTG rushScientacking83 helpedborgTG objectiveectiveо borrowersolescentة rebirthTG ensures cigarettes totally libraries glorybec stripedholeolescent pianoricalirling rampage Faction investigated reviewers DepartmentTGolescentollah Renew presently Productions piracy unresウ▄ Wealth408 Renoicultural\n","Iteration 1/1500\r"]}],"source":["num_epochs = 10\n","vocab_size = 50295\n","\n","phi2_projection_model.train()\n","N_batches = len(train_dataloader)\n","                \n","for epoch in range(num_epochs):\n","\n","    print(f\"Working on epoch {epoch}\")\n","\n","    for iteration, batch in enumerate(train_dataloader):\n","\n","        if iteration == num_batches_train_on: \n","            break \n","\n","        print(f\"Iteration {iteration}/{num_batches_train_on}\", end='\\r')\n","\n","        optimizer.zero_grad()\n","\n","        input = batch[0]\n","        gt = batch[1] \n","\n","        output = phi2_projection_model(input.to(device))\n","\n","        ## need to map gt token_ids to one-hot enocding vocab_size\n","        gt_one_hot = torch.nn.functional.one_hot(gt, vocab_size).to(torch.float32)\n","\n","        ## output in correct shape\n","        output_tensor_new = torch.empty(batch_size_train, max_token_len_data, vocab_size)\n","        for idx, s in enumerate(output.scores): \n","            output_tensor_new[:, idx, :] = s\n","        \n","        output_tensor_new = F.softmax(output_tensor_new, dim=-1)\n","        \n","        ## ce loss between output_tensor_new and gt_one_hot\n","        loss = F.cross_entropy(output_tensor_new.view(-1, vocab_size), gt.view(-1))\n","\n","        loss.requires_grad = True\n","        loss.backward()\n","\n","        optimizer.step() \n","        # optimizer.zero_grad(set_to_none=True) \n","\n","        if (iteration % 100) == 0: \n","            print(\"\")\n","\n","            ## print gt and output decoded tokens for visual inspection for the 1st el of batch\n","            gt_input_ids = gt[0]\n","            output_input_ids = torch.argmax(output_tensor_new[0], dim=1)\n","            output_sequences_input_ids = output.sequences[0, :]\n","\n","            gt_idx_0_decoded = tokenizer.decode(gt_input_ids).replace('<|endoftext|>', '')\n","            output_idx_0_decoded = tokenizer.decode(output_input_ids).replace('<|endoftext|>', '')\n","            output_sequences_idx_0_decoded = tokenizer.decode(output_sequences_input_ids).replace('<|endoftext|>', '')\n","\n","            # print(f\"Loss: {loss}\\nInput_ids (gt): {gt_input_ids}\\nInput_ids (pred): {output_input_ids}\\nInput_ids (sequence): {output_sequences_input_ids}\")\n","            ## print loss \n","            print(f\"Loss: {loss}\\nCaption (gt): {gt_idx_0_decoded}\\nCaption (pred): {output_idx_0_decoded}\\nCaption (sequence): {output_sequences_idx_0_decoded}\")\n"," \n","        \n","    print(\"\")\n","    print(f\"Epoch {epoch} finished\")\n","    print(\"\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
