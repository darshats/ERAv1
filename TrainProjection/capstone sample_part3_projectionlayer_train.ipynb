{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd8f9359587b410587783bd69fafc3bc","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"microsoft/phi-2\"\n","phi2_model_pretrained = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True\n",")"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[{"data":{"text/plain":["PhiForCausalLM(\n","  (model): PhiModel(\n","    (embed_tokens): Embedding(51200, 2560)\n","    (embed_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-31): 32 x PhiDecoderLayer(\n","        (self_attn): PhiAttention(\n","          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n","          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n","          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n","          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n","          (rotary_emb): PhiRotaryEmbedding()\n","        )\n","        (mlp): PhiMLP(\n","          (activation_fn): NewGELUActivation()\n","          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n","          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n","        )\n","        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",")"]},"execution_count":140,"metadata":{},"output_type":"execute_result"}],"source":["phi2_model_pretrained"]},{"cell_type":"markdown","metadata":{},"source":["### Create model "]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class SimpleResBlock(nn.Module):\n","    def __init__(self, input_size):\n","        super().__init__()\n","        self.pre_norm = nn.LayerNorm(input_size)\n","        self.proj = nn.Sequential(\n","            nn.Linear(input_size, input_size),\n","            nn.GELU(),\n","            nn.Linear(input_size, input_size)\n","        )\n","    def forward(self, x):\n","        x = self.pre_norm(x)\n","        return x + self.proj(x)\n","    \n","class Phi2wrapper(nn.Module):\n","    \n","    #This defines the structure of the NN.\n","    def __init__(self, input_dim_CLIP=768, input_dim_phi2=2560, phi2_model=phi2_model_pretrained):\n","        super(Phi2wrapper, self).__init__()\n","        self.input_dim_CLIP = 768\n","        self.input_dim_phi2 = 2560\n","        self.projection_img = nn.Linear(self.input_dim_CLIP, self.input_dim_phi2, \n","                                        bias=False)\n","        self.resblock = SimpleResBlock(self.input_dim_phi2)\n","        self.phi2_model = phi2_model\n","\n","    def forward(self, x):\n","\n","        x = self.projection_img(x)\n","        x = self.resblock(x)\n","\n","        x = self.phi2_model.model.layers[0](x)\n","        for layer_idx in range(1, 32): \n","            x = self.phi2_model.model.layers[layer_idx](x[0])\n","                \n","        x = self.phi2_model.model.final_layernorm(x[0])\n","        x = self.phi2_model.lm_head(x)\n","        \n","        return x "]},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[],"source":["phi2_projection_model = Phi2wrapper()"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[],"source":["## Freezing phi-2 for projection layer training \n","\n","for name, param in phi2_projection_model.named_parameters():\n","    if \"phi2_model\" in name:\n","        param.requires_grad = False"]},{"cell_type":"markdown","metadata":{},"source":["### Create dataset"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import pandas as pd \n","import json\n","import os \n","import h5py\n"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[],"source":["def get_image_name(image_id_from_caption, list_image_info): \n","    for img in list_image_info: \n","        if img['id'] == image_id_from_caption: \n","            img_name = img['file_name'].split('.')[0]\n","            return img['file_name'].split('.')[0]\n","    return 'NoImgNameFound'"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[],"source":["# file_path_captions_coco = '/media/App/amaranth/lavanya/Capstone_data/annotations_trainval2017/annotations/captions_train2017.json'\n","\n","# with open(file_path_captions_coco) as f:\n","#    data = json.load(f)\n","\n","# captions_info = []\n","# for a in data['annotations']: \n","#     captions_info.append([a['image_id'], a['caption'], a['id']])\n","\n","# captions_info_df = pd.DataFrame(data=captions_info, columns=['image_id', 'caption', 'caption_id'])\n","# captions_info_df['image_name'] = captions_info_df['image_id'].apply(lambda x: get_image_name(x, data['images']))\n","# captions_info_df['image_name'] = captions_info_df['image_name'].apply(lambda x: '0'*(12-len(str(x))) + str(x))\n","# captions_info_df.to_csv('captions_images_map_COCO_train2017.csv')"]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_3285481/1237926302.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n","  captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')\n"]}],"source":["captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[],"source":["import h5py    \n","import numpy as np    "]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[],"source":["class COCO_CLIP_Dataset(Dataset):\n","    def __init__(\n","        self, caption_file, embedding_path):\n","        self.embedding_path = embedding_path\n","        self.caption_file = caption_file\n","\n","    def __len__(self):\n","        return len(self.caption_file)\n","    \n","    def __getitem__(self, index):\n","        row = self.caption_file.iloc[[index]]\n","        df_img = row['image_name'].values[0]\n","        img_base_name = '0'*(12-len(str(df_img))) + str(df_img)\n","        img_base_name = img_base_name.replace(' ', '0')\n","        img_clip_embedding_path = os.path.join(self.embedding_path, f'{img_base_name}.h5')\n","\n","        np_array_embed_img = h5py.File(img_clip_embedding_path,'r+')['image_features'][()]\n","        \n","        img_caption = row['caption'] ## Tokenize this \n","\n","        return torch.tensor(np_array_embed_img)"]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[],"source":["dataset = COCO_CLIP_Dataset(captions_info_df, '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/')"]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 49, 51200])"]},"execution_count":183,"metadata":{},"output_type":"execute_result"}],"source":["phi2_projection_model(dataset[0]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
