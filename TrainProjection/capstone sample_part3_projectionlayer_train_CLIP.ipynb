{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c28132572047bbb7ed5a1e135f1bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import  AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "phi2_model_pretrained = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "import json\n",
    "import os \n",
    "import h5py\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1469604/1237926302.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')\n"
     ]
    }
   ],
   "source": [
    "captions_info_df = pd.read_csv('captions_images_map_COCO_train2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCO_CLIP_Dataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, caption_file, embedding_path, tokenizer, max_token_len_data, phi2_model_pretrained, max_seq_len):\n",
    "        \n",
    "        self.embedding_path = embedding_path\n",
    "        self.caption_file = caption_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len_data = max_token_len_data\n",
    "        self.phi2_model = phi2_model_pretrained\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_file)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        row = self.caption_file.iloc[[index]]\n",
    "\n",
    "        df_img = row['image_id'].values[0]\n",
    "        img_base_name = '0'*(12-len(str(df_img))) + str(df_img)\n",
    "        img_base_name = img_base_name.replace(' ', '0')\n",
    "        img_clip_embedding_path = os.path.join(self.embedding_path, f'{img_base_name}.h5')\n",
    "\n",
    "        np_array_embed_img = h5py.File(img_clip_embedding_path,'r+')['image_features'][()]\n",
    "        \n",
    "        img_caption = row['caption'].values[0] ## Tokenize this \n",
    "        img_caption_tokenized = self.tokenizer(img_caption, return_tensors=\"pt\", \n",
    "                                               return_attention_mask=False).input_ids\n",
    "        pad_len = self.max_seq_len - img_caption_tokenized.shape[1]\n",
    "        if pad_len != 0: \n",
    "            pad_tokens = torch.tensor([self.tokenizer.eos_token_id]*pad_len).unsqueeze(0)\n",
    "            img_caption_tokenized = torch.cat((img_caption_tokenized, pad_tokens), dim=-1)\n",
    "            \n",
    "        img_caption_embedding = self.phi2_model.get_input_embeddings()(img_caption_tokenized)\n",
    "        \n",
    "        return torch.tensor(np_array_embed_img).squeeze(0), img_caption_embedding.squeeze(0).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(image_id, fpath = '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/'): \n",
    "\n",
    "    n = '0'*(12-len(str(image_id))) + str(image_id) + '.h5'\n",
    "    fp = os.path.join(fpath, n)\n",
    "\n",
    "    if os.path.exists(fp): \n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### captions_info_df contains for 1 image multiple entries, lets reduce keeping one image, one entry. \n",
    "captions_info_df_subset = captions_info_df.drop_duplicates(subset='image_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_len_data = 75\n",
    "phi2_embed_dim = 2560\n",
    "clip_embed_patch = 768\n",
    "clip_embed_token= 49 \n",
    "\n",
    "dataset = COCO_CLIP_Dataset(captions_info_df_subset, \n",
    "                            '/media/App/amaranth/lavanya/Capstone_data/clip_features_base_patch32/', \n",
    "                            tokenizer, max_token_len_data, phi2_model_pretrained, max_token_len_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, clip_embed_patch=clip_embed_patch, \n",
    "#                  max_seq_len=max_token_len_data, \n",
    "#                  phi2_embed_dim=phi2_embed_dim): \n",
    "        \n",
    "#         super(MyModel, self).__init__()\n",
    "        \n",
    "#         self.clip_embed_patch = clip_embed_patch\n",
    "#         self.max_seq_len = max_seq_len\n",
    "#         self.phi2_embed_dim = phi2_embed_dim\n",
    "        \n",
    "#         # Global Average Pooling layer\n",
    "#         self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "#         # Linear layer\n",
    "#         self.fc1 = nn.Linear(self.clip_embed_patch, 3000)\n",
    "#         self.fc2 = nn.Linear(3000, 3000)\n",
    "#         self.fc3 = nn.Linear(3000, self.max_seq_len*self.phi2_embed_dim)\n",
    "\n",
    "#         # Optional activation functions\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):        \n",
    "#         # Global Average Pooling\n",
    "#         x = self.global_avg_pooling(x.transpose(1, 2)).squeeze(dim=2)\n",
    "\n",
    "#         # Linear layer\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "\n",
    "#         x = x.view(-1, self.max_seq_len, self.phi2_embed_dim)\n",
    "        \n",
    "#         return x\n",
    "        \n",
    "\n",
    "# model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResBlock(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.pre_norm = nn.LayerNorm(input_size)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, input_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.pre_norm(x)\n",
    "        return x + self.proj(x)\n",
    "    \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, clip_embed_patch=clip_embed_patch, clip_embed_token = clip_embed_token,\n",
    "                 max_seq_len=max_token_len_data, \n",
    "                 phi2_embed_dim=phi2_embed_dim): \n",
    "        \n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.clip_embed_patch = clip_embed_patch\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.phi2_embed_dim = phi2_embed_dim\n",
    "        self.clip_embed_token = clip_embed_token\n",
    "        \n",
    "        self.linear_1 = nn.Linear(self.clip_embed_patch, 1500) \n",
    "        self.linear_2 = nn.Linear(1500, 1500)\n",
    "        self.linear_3 = nn.Linear(1500, self.phi2_embed_dim)\n",
    "        \n",
    "        self.projection_1 = SimpleResBlock(self.phi2_embed_dim)   \n",
    "        \n",
    "        self.fc4 = nn.Linear(self.clip_embed_token, self.max_seq_len)\n",
    "\n",
    "        # Optional activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):        \n",
    "        # -1, 49, 768, --> -1, 49, 2560 \n",
    "        x = self.relu(self.linear_3(self.relu(self.linear_2(self.relu(self.linear_1(x))))))    \n",
    "        \n",
    "        x = self.projection_1(x)  # -1, 49, 2560, --> -1, 49, 2560\n",
    "        \n",
    "        x = x.swapaxes(-2, -1)    # -1, 2560, 49\n",
    "        x = self.fc4(x)           # -1, 2560, 49 --> -1, 2560, 75\n",
    "        \n",
    "        x = x.swapaxes(-2, -1)    # -1, 75, 2560\n",
    "        x = self.projection_1(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 32\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size_train, shuffle=True, num_workers=8)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-5, eps=1e-9) \n",
    "normalize = transforms.Normalize(mean = 0, std = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime \n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"BiggerRemoveGAP{datetime.now().strftime('%b%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on epoch 0\n",
      "Loss: tensor(0.9995, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9962, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9928, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9888, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9859, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9816, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9790, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9760, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9719, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9682, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9676, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9606, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9577, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9531, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9521, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9496, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9451, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9426, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9379, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9319, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9306, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9280, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9228, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9220, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9194, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9131, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9124, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9106, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.9056, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8982, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8976, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8935, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8950, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8843, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8818, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8816, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8761, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8729, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8721, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8685, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8669, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8595, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8571, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8587, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8551, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8511, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8458, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8419, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8414, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8330, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8377, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8372, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8329, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8314, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8247, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8232, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8254, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8233, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8204, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8100, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8129, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8115, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8079, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8023, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8061, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.8034, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7984, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7981, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7925, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7946, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7953, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7968, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7892, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7960, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7902, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7866, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7895, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7856, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7888, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7811, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7755, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7883, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7773, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7829, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7742, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7777, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7739, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7772, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7705, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7705, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7748, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7753, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7705, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7764, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7635, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7609, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7715, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7634, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7639, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7715, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7574, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7730, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7654, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7690, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7636, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7686, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7613, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7621, grad_fn=<RsubBackward1>)\n",
      "Loss: tensor(0.7540, grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "count = 0 \n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs): \n",
    "    \n",
    "    print(f\"Working on epoch {epoch}\")\n",
    "    for iteration, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        input_ = batch[0]\n",
    "        gt_ = batch[1]\n",
    "          \n",
    "        output_ = model(input_)\n",
    "                \n",
    "        cosine_sim = F.cosine_similarity(output_, gt_).mean()\n",
    "        loss = 1 - cosine_sim\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss:\", loss)\n",
    "        writer.add_scalar('Loss/train', loss, count)\n",
    "        count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(phi2_model_pretrained.generate(inputs_embeds=output_[0, :, :].unsqueeze(0), bos_token_id=tokenizer.bos_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(phi2_model_pretrained.generate(inputs_embeds=gt_[0, :, :].unsqueeze(0), bos_token_id=tokenizer.bos_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
